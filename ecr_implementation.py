# -*- coding: utf-8 -*-
"""ECR_implementation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EfjwGF_wC4DtDteSKo1DKbP4zE2oqmsa
"""

import torch
from torch import nn


class ECR(nn.Module):
  def __init__(self, weight_loss_ECR, sinkhorn_alpha, OT_max_iter=5000, stopThr=.5e-2):
    super().__init__()

    self.weight_loss_ECR = weight_loss_ECR
    self.sinkhorn_alpha = sinkhorn_alpha
    self.OT_max_iter = OT_max_iter
    self.stopThr = stopThr
    self.epsilon = 1e-16

  def forward(self, M):
    # M: KxV -> Cost Matrix
    # a: Kx1 -> Mass(distribution) at source locations -> Sum to 1
    # b: Vx1 -> Mass(distribution) at destination locations -> Sum to 1

    device = M.device

    #sinkhorn algorithm
    a = (torch.ones(M.shape[0]) / M.shape[0]).unsqueeze(1).to(device) # assign equally
    b = (torch.ones(M.shape[1]) / M.shape[1]).unsqueeze(1).to(device)

    u = (torch.ones_like(a) / a.size()[0]).to(device) # Kx1 -> initialize uniformly

    K = torch.exp(-M * self.sinkhorn_alpha) # Converts M into a similarity-based kernel

    err = 1
    cpt = 0
    while err > self.stopThr and cpt < self.OT_max_iter:
      cpt += 1
      v = torch.div(b, torch.matmul(K.t(), u) + self.epsilon)
      u = torch.div(a, torch.matmul(K, v) + self.epsilon)
      if cpt % 50 == 1:
        cur_b = torch.mul(v, torch.matmul(K.t(), u))
        err = torch.norm(torch.sum(torch.abs(b_cur - b), dim=0), p=float('inf'))

      transp = u * (K * v.T) # .T used for all dimension while .t() just used for 2D

      loss_ECR = torch.sum(transp * M)
      loss_ECR = self.weight_loss_ECR * loss_ECR

    return loss_ECR